{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch_scatter import scatter_add\n",
    "# from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "# from torch_geometric.nn.conv import MessagePassing\n",
    "# from torch_geometric.utils import add_remaining_self_loops\n",
    "# from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "# from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dict = load_obj('AAL_data/timeseries/AD')\n",
    "CN_dict = load_obj('AAL_data/timeseries/CN')\n",
    "\n",
    "AD_train = load_obj('AAL_data/AD_train_full')\n",
    "AD_val = load_obj('AAL_data/AD_val_full')\n",
    "AD_test = load_obj('AAL_data/AD_test_full')\n",
    "\n",
    "CN_train = load_obj('AAL_data/CN_train_full')\n",
    "CN_val = load_obj('AAL_data/CN_val_full')\n",
    "CN_test = load_obj('AAL_data/CN_test_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(timeseries,msr):\n",
    "    correlation_measure = ConnectivityMeasure(kind=msr)\n",
    "    correlation_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "    return correlation_matrix\n",
    "\n",
    "def get_upper_triangular_matrix(matrix):\n",
    "    upp_mat = []\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(i+1,len(matrix)):\n",
    "            upp_mat.append(matrix[i][j])\n",
    "    return upp_mat\n",
    "\n",
    "    \n",
    "# def get_adj_mat(correlation_matrix, threshold_value, weighted = True):\n",
    "#     adj_mat = []\n",
    "#     for i in correlation_matrix:\n",
    "#         row = []\n",
    "#         for j in i:\n",
    "#             if abs(j)>threshold_value:\n",
    "#                 if not weighted:\n",
    "#                     row.append(1)\n",
    "#                 else:\n",
    "#                     row.append(abs(j))\n",
    "#             else:\n",
    "#                 row.append(0)\n",
    "#         adj_mat.append(row)\n",
    "#     return adj_mat\n",
    "\n",
    "\n",
    "def get_adj_mat(correlation_matrix, th_value_p, th_value_n):\n",
    "    adj_mat = []\n",
    "    k=0\n",
    "    for i in correlation_matrix:\n",
    "        row = []\n",
    "        for j in i:\n",
    "            if j>0:\n",
    "                if j > th_value_p:\n",
    "                    row.append(j)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            else:\n",
    "                if abs(j) > th_value_n:\n",
    "                    row.append(abs(j))\n",
    "                else:\n",
    "                    row.append(0)\n",
    "        adj_mat.append(row)\n",
    "#     adj_mat = connect_isolated_nodes(adj_mat, correlation_matrix)\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "def connect_isolated_nodes(adj_mat, correlation_matrix):\n",
    "    correlation_matrix = list(np.array(correlation_matrix) - np.array(np.eye(len(correlation_matrix))))\n",
    "    correlation_matrix = [list(a) for a in correlation_matrix]\n",
    "                              \n",
    "    for row_num in range(len(adj_mat)):\n",
    "        if sum(adj_mat[row_num]) == 0:\n",
    "            index_max_element_corr_row = correlation_matrix[row_num].index(max(correlation_matrix[row_num]))\n",
    "            adj_mat[row_num][index_max_element_corr_row] = 1\n",
    "    return adj_mat\n",
    "    \n",
    "# def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "#     ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "#     cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "#     ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "#     cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "#     all_correlation_values = ad_upper + cn_upper\n",
    "#     all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "\n",
    "#     all_correlation_values = np.array([abs(i) for i in all_correlation_values])\n",
    "#     all_correlation_values = np.sort(all_correlation_values)[::-1]\n",
    "\n",
    "#     th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "#     return all_correlation_values[int(th_val_index)]\n",
    "\n",
    "\n",
    "def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "    ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "    cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "    ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "    cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "    all_correlation_values = ad_upper + cn_upper\n",
    "    all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "    \n",
    "    all_correlation_values_pos=[]\n",
    "    all_correlation_values_neg=[]\n",
    "    for i in all_correlation_values:\n",
    "        if i==1:\n",
    "            continue\n",
    "        elif i>0:\n",
    "            all_correlation_values_pos.append(i)\n",
    "        else:  \n",
    "            all_correlation_values_neg.append(abs(i))\n",
    "\n",
    "    all_correlation_values_pos = np.array(all_correlation_values_pos)\n",
    "    all_correlation_values_pos = np.sort(all_correlation_values_pos)[::-1]\n",
    "    \n",
    "    all_correlation_values_neg = np.array(all_correlation_values_neg)\n",
    "    all_correlation_values_neg = np.sort(all_correlation_values_neg)[::-1]\n",
    "\n",
    "    th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "    \n",
    "    return all_correlation_values_pos[int(th_val_index)], all_correlation_values_neg[int(th_val_index)]\n",
    "\n",
    "def create_graph(timeseries, threshold_p, threshold_n, y, measure='correlation'):\n",
    "    correlation_matrix = get_correlation_matrix(timeseries, measure)\n",
    "    adj_mat = get_adj_mat(correlation_matrix, threshold_p, threshold_n)\n",
    "\n",
    "    G = nx.from_numpy_matrix(np.array(adj_mat), create_using=nx.DiGraph)\n",
    "    data=torch_geometric.utils.from_networkx(G)\n",
    "    data['x'] = torch.tensor(correlation_matrix, dtype=torch.float)\n",
    "    data['y'] = torch.tensor([y])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "        data = data.to(device)\n",
    "        return data\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_features = torch.tensor([get_upper_triangular_matrix(get_correlation_matrix(AD_dict[sub_id],'correlation')) for sub_id in AD_dict])\n",
    "CN_features = torch.tensor([get_upper_triangular_matrix(get_correlation_matrix(CN_dict[sub_id],'correlation')) for sub_id in CN_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_loader = DataLoader(AD_features, batch_size=32, shuffle=True)\n",
    "CN_loader = DataLoader(CN_features, batch_size=32, shuffle=True)\n",
    "# for batch_idx, real in enumerate(CN_loader):\n",
    "#     print(batch_idx)\n",
    "#     print(real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, dim = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1)).repeat(1, dim).to(device)\n",
    "    interpolated_mats = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_mats)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_mats,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(features_dim, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(4096, 2048),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(2048, 1024),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(512, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, features_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(1024, 2048),\n",
    "#             nn.LeakyReLU(0.1),\n",
    "#             nn.Linear(2048, 4096),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, features_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-6\n",
    "features_dim = 6670     # 116*116 connectivity matrices\n",
    "noise_dim = 100\n",
    "batch_size = 32\n",
    "num_epochs = 2000\n",
    "LAMBDA_GP = 10\n",
    "Dloss = []\n",
    "Gloss = []\n",
    "\n",
    "disc = Discriminator(features_dim).to(device)\n",
    "gen = Generator(noise_dim, features_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, noise_dim)).to(device)\n",
    "# transforms = transforms.Compose(\n",
    "#     [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
    "# )\n",
    "\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    D_l = 0\n",
    "    G_l = 0\n",
    "    cnt = 0\n",
    "    for batch_idx, real in enumerate(CN_loader):\n",
    "        real = real.view(-1, 6670).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real.float()).view(-1)\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        \n",
    "        gp = gradient_penalty(disc, real.float(), fake, device=device)\n",
    "        lossD = -(torch.mean(disc_real) - torch.mean(disc_fake)) + LAMBDA_GP * gp\n",
    "        D_l += lossD * batch_size\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        output = disc(fake)\n",
    "        lossG = -torch.mean(fake)\n",
    "        G_l += lossG * batch_size\n",
    "        \n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        cnt += batch_size\n",
    "    \n",
    "    D_l /= cnt\n",
    "    G_l /= cnt\n",
    "    \n",
    "    Gloss.append(G_l)\n",
    "    Dloss.append(D_l)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Loss D: {D_l:.4f}, loss G: {lossG:.4f}\")\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "#                 data = real.reshape(-1, 1, 28, 28)\n",
    "#                 img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "#                 img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "#                 writer_fake.add_image(\n",
    "#                     \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "#                 )\n",
    "#                 writer_real.add_image(\n",
    "#                     \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "#                 )\n",
    "#                 step += 1\n",
    "plt.plot(Gloss,label='Gen')\n",
    "plt.plot(Dloss,label='Disc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Gloss,label='Gen')\n",
    "plt.plot(Dloss,label='Disc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, B).to(device)\n",
    "    interpolated_mat = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    mixed_scores = critic(interpolated_mat)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_mat,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "#             critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "            \n",
    "#         for _ in range(CRITIC_ITERATIONS):\n",
    "#             noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "#             fake = gen(noise)\n",
    "# #             critic_real = critic(real).reshape(-1)\n",
    "#             critic_fake = critic(fake).reshape(-1)\n",
    "#             gp = gradient_penalty(critic, real, fake, device=device)\n",
    "#             loss_critic = (\n",
    "#                 -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "#             )\n",
    "#             critic.zero_grad()\n",
    "#             loss_critic.backward(retain_graph=True)\n",
    "#             opt_critic.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
