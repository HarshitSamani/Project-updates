{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dict = load_obj('AAL_data/timeseries/AD')\n",
    "CN_dict = load_obj('AAL_data/timeseries/CN')\n",
    "\n",
    "AD_train = load_obj('AAL_data/samples/AD_train')\n",
    "AD_val = load_obj('AAL_data/samples/AD_val')\n",
    "AD_test = load_obj('AAL_data/samples/AD_test')\n",
    "\n",
    "CN_train = load_obj('AAL_data/samples/CN_train')\n",
    "CN_val = load_obj('AAL_data/samples/CN_val')\n",
    "CN_test = load_obj('AAL_data/samples/CN_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(timeseries,msr):\n",
    "    correlation_measure = ConnectivityMeasure(kind=msr)\n",
    "    correlation_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "def create_graph(timeseries, y, measure = 'correlation'):\n",
    "    correlation_matrix = get_correlation_matrix(timeseries, measure)\n",
    "    adj_mat = abs(correlation_matrix)\n",
    "\n",
    "    G = nx.from_numpy_matrix(np.array(adj_mat), create_using=nx.DiGraph)\n",
    "    data=torch_geometric.utils.from_networkx(G)\n",
    "    data['x'] = torch.tensor(correlation_matrix, dtype=torch.float)\n",
    "    data['y'] = torch.tensor([y])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        data = data.to(device)\n",
    "        return data\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, dim1, num_hidden_channels, hidden_channels_dims,initial_thr): ## 'initial_thr' new parameter\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.hidden_channels_dims = hidden_channels_dims\n",
    "        self.conv1 = GCNConv(dim1, hidden_channels_dims[0])\n",
    "        self.conv2 = GCNConv(hidden_channels_dims[0], hidden_channels_dims[1])\n",
    "        self.conv3 = GCNConv(hidden_channels_dims[1], hidden_channels_dims[2])\n",
    "        self.lin1 = Linear(hidden_channels_dims[-1], 2)\n",
    "        \n",
    "        self.threshold = torch.nn.Parameter(torch.tensor(initial_thr, requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, x1, edge_index1, edge_weight1, batch1):\n",
    "        \n",
    "        edge_weight1 = edge_weight1*torch.sigmoid(50*(edge_weight1 - self.threshold))\n",
    "#         edge_weight1 = F.relu(edge_weight1 - self.threshold)\n",
    "    \n",
    "        x1= self.conv1(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv2(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv3(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "\n",
    "        x1 = global_mean_pool(x1, batch1)\n",
    "        x1 = self.lin1(x1)\n",
    "        x1 = torch.softmax(x1,dim=1)\n",
    "\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation acc doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation acc improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation acc improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float(\"inf\")\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  \n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "def find_loss(loader):\n",
    "    model.eval()\n",
    "    func = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    loss = 0\n",
    "    for i in loader:\n",
    "        out = model(i.x, i.edge_index, i.weight, i.batch)\n",
    "        loss += func(out, i.y).item()\n",
    "    return loss/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(dim1 = 116, num_hidden_channels = 2, hidden_channels_dims = [32, 24,16], initial_thr = 0.0)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "(model.threshold.item() + 5)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_graph={}\n",
    "CN_graph={}\n",
    "for sub_id in (AD_dict):\n",
    "    AD_graph[sub_id] = create_graph(timeseries=AD_dict[sub_id],y=0)\n",
    "for sub_id in (CN_dict):\n",
    "    CN_graph[sub_id] = create_graph(timeseries=CN_dict[sub_id],y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_channels = 3\n",
    "hidden_channels_dims = [32,24,16]\n",
    "activation = 'Relu'\n",
    "Patience = 20\n",
    "Dropout = 0.5 # Dropout removed\n",
    "Learning_rate = 0.001\n",
    "\n",
    "for init_th in [0.1,0.3,0.5,0.7,0.9]:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    final_train_accs = []\n",
    "    final_test_accs = []\n",
    "    final_val_accs = []\n",
    "\n",
    "    final_ths = []\n",
    "    \n",
    "    seeds=100\n",
    "    for i in range(1,seeds+1):\n",
    "        # print(i)\n",
    "        random.seed(i)\n",
    "        np.random.seed(i)\n",
    "\n",
    "        train_data = [AD_graph[sub_id] for sub_id in AD_train[i-1]] + [CN_graph[sub_id] for sub_id in CN_train[i-1]]\n",
    "        train_label = [0]*len(AD_train[i-1]) + [1]*len(CN_train[i-1])\n",
    "        train_data,train_label = shuffle(train_data, train_label, random_state=i)\n",
    "        \n",
    "        val_data = [AD_graph[sub_id] for sub_id in AD_val[i-1]] + [CN_graph[sub_id] for sub_id in CN_val[i-1]]\n",
    "        val_label = [0]*len(AD_val[i-1]) + [1]*len(CN_val[i-1])\n",
    "        val_data, val_label = shuffle(val_data, val_label, random_state=i)\n",
    "\n",
    "        test_data = [AD_graph[sub_id] for sub_id in AD_test[i-1]] + [CN_graph[sub_id] for sub_id in CN_test[i-1]]\n",
    "        test_label = [0]*len(AD_test[i-1]) + [1]*len(CN_test[i-1])\n",
    "        test_data,test_label = shuffle(test_data, test_label, random_state=i)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims, initial_thr = init_th)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        epochs=200\n",
    "        \n",
    "        os.makedirs(f\"train_thr_VAL_LOSS_SIGMOID/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{init_th}\", exist_ok=True)\n",
    "        checkpt_path = f\"train_thr_VAL_LOSS_SIGMOID/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{init_th}/checkpoint_seed_{i}.pt\"\n",
    "        early_stopping = EarlyStopping(patience=Patience, verbose=True, path=checkpt_path)\n",
    "        \n",
    "\n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            train(model)\n",
    "            train_loss = find_loss(train_loader)\n",
    "            val_loss = find_loss(val_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch:{epoch}\")\n",
    "#                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "                print(f'GCN graph classification Epoch: {epoch:03d}, Train Acc: {test(train_loader):.4f}, Val Acc: {test(val_loader):.4f}')\n",
    "                break\n",
    "            if epoch==epochs:\n",
    "                print(f'GCN graph classification Epoch: {epoch:03d}, Train Acc: {test(train_loader):.4f}, Val Acc: {test(val_loader):.4f}')\n",
    "#                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "        plt.plot(train_losses,label='train')\n",
    "        plt.plot(val_losses,label='val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print('seed no.:',i)\n",
    "#         print(\"GCN Final accuracy average = \", sum(final_val_accs)/len(final_val_accs))\n",
    "#         print('\\n\\n')\n",
    "        \n",
    "        model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims, initial_thr = init_th)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            model.to(device)\n",
    "        checkpt_path = f\"train_thr_VAL_LOSS_SIGMOID/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{init_th}\"\n",
    "        model.load_state_dict(torch.load(checkpt_path+f\"/checkpoint_seed_{i}.pt\"))\n",
    "        final_train_accs.append(test(train_loader))\n",
    "        final_val_accs.append(test(val_loader))\n",
    "        final_test_accs.append(test(test_loader))\n",
    "        final_ths.append(model.threshold.item())\n",
    "\n",
    "    with open(f\"train_thr_VAL_LOSS_SIGMOID/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{init_th}/report\", \"w\") as file:\n",
    "        file.write(f\"num_hidden_channels={num_hidden_channels}\\n\"\n",
    "                   f\"hidden_channels_dims={hidden_channels_dims}\\n\"\n",
    "                   f\"Activation={activation}\\n\"\n",
    "                   f\"Patience={Patience}\\n\"\n",
    "                   f\"Dropout={Dropout}\\n\"\n",
    "                   f\"Learning_rate={Learning_rate}\\n\"\n",
    "                   f\"seeds={seeds}\\n\"\n",
    "                   f\"epochs={epochs}\\n\"\n",
    "                   f\"Initial_threshold={init_th}%\\n\"\n",
    "                   f\"train_avg_accuracy={stat.mean(final_train_accs)*100:0.2f}%\\n\"\n",
    "                   f\"train_accuracy_stdev={stat.stdev(final_train_accs)*100:0.2f}%\\n\"\n",
    "                   f\"val_avg_accuracy={stat.mean(final_val_accs)*100:0.2f}%\\n\"\n",
    "                   f\"val_accuracy_stdev={stat.stdev(final_val_accs)*100:0.2f}%\\n\"\n",
    "                   f\"test_avg_accuracy={stat.mean(final_test_accs)*100:0.2f}%\\n\"\n",
    "                   f\"test_accuracy_stdev={stat.stdev(final_test_accs)*100:0.2f}%\\n\"\n",
    "                   f\"threshold_avg={stat.mean(final_ths)}\\n\"\n",
    "                   f\"threshold_stdev={stat.stdev(final_ths)}\\n\"\n",
    "                   f\"threshold_min={min(final_ths)}\\n\"\n",
    "                   f\"threshold_max={max(final_ths)}\\n\")\n",
    "    end = time.time()\n",
    "    with open(\"time_report\",\"a\") as f:\n",
    "        f.write(f\"time_{init_th} = {end-start:0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"jhj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
