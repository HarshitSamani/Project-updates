{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dict = load_obj('AAL_data/timeseries/AD')\n",
    "CN_dict = load_obj('AAL_data/timeseries/CN')\n",
    "\n",
    "AD_train = load_obj('AAL_data/samples/AD_train')\n",
    "AD_val = load_obj('AAL_data/samples/AD_val')\n",
    "AD_test = load_obj('AAL_data/samples/AD_test')\n",
    "\n",
    "CN_train = load_obj('AAL_data/samples/CN_train')\n",
    "CN_val = load_obj('AAL_data/samples/CN_val')\n",
    "CN_test = load_obj('AAL_data/samples/CN_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(timeseries,msr):\n",
    "    correlation_measure = ConnectivityMeasure(kind=msr)\n",
    "    correlation_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "def get_upper_triangular_matrix(matrix):\n",
    "    upp_mat = []\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(i+1,len(matrix)):\n",
    "            upp_mat.append(matrix[i][j])\n",
    "    return upp_mat\n",
    "\n",
    "\n",
    "def get_adj_mat(correlation_matrix, th_value_p, th_value_n):\n",
    "    adj_mat = []\n",
    "    k=0\n",
    "    for i in correlation_matrix:\n",
    "        row = []\n",
    "        for j in i:\n",
    "            if j>0:\n",
    "                if j > th_value_p:\n",
    "                    row.append(j)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            else:\n",
    "                if abs(j) > th_value_n:\n",
    "                    row.append(abs(j))\n",
    "                else:\n",
    "                    row.append(0)\n",
    "        adj_mat.append(row)\n",
    "#     adj_mat = connect_isolated_nodes(adj_mat, correlation_matrix)\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "def connect_isolated_nodes(adj_mat, correlation_matrix):\n",
    "    correlation_matrix = list(np.array(correlation_matrix) - np.array(np.eye(len(correlation_matrix))))\n",
    "    correlation_matrix = [list(a) for a in correlation_matrix]\n",
    "                              \n",
    "    for row_num in range(len(adj_mat)):\n",
    "        if sum(adj_mat[row_num]) == 0:\n",
    "            index_max_element_corr_row = correlation_matrix[row_num].index(max(correlation_matrix[row_num]))\n",
    "            adj_mat[row_num][index_max_element_corr_row] = 1\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "    ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "    cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "    ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "    cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "    all_correlation_values = ad_upper + cn_upper\n",
    "    all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "    \n",
    "    all_correlation_values_pos=[]\n",
    "    all_correlation_values_neg=[]\n",
    "    for i in all_correlation_values:\n",
    "        if i==1:\n",
    "            continue\n",
    "        elif i>0:\n",
    "            all_correlation_values_pos.append(i)\n",
    "        else:  \n",
    "            all_correlation_values_neg.append(abs(i))\n",
    "\n",
    "    all_correlation_values_pos = np.array(all_correlation_values_pos)\n",
    "    all_correlation_values_pos = np.sort(all_correlation_values_pos)[::-1]\n",
    "    \n",
    "    all_correlation_values_neg = np.array(all_correlation_values_neg)\n",
    "    all_correlation_values_neg = np.sort(all_correlation_values_neg)[::-1]\n",
    "\n",
    "    th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "    \n",
    "    return all_correlation_values_pos[int(th_val_index)], all_correlation_values_neg[int(th_val_index)]\n",
    "\n",
    "\n",
    "def create_graph(timeseries, threshold_p, threshold_n, y, measure='correlation'):\n",
    "    correlation_matrix = get_correlation_matrix(timeseries, measure)\n",
    "    adj_mat = get_adj_mat(correlation_matrix, threshold_p, threshold_n)\n",
    "\n",
    "    G = nx.from_numpy_matrix(np.array(adj_mat), create_using=nx.DiGraph)\n",
    "    data=torch_geometric.utils.from_networkx(G)\n",
    "    data['x'] = torch.tensor(correlation_matrix, dtype=torch.float)\n",
    "    data['y'] = torch.tensor([y])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        data = data.to(device)\n",
    "        return data\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, dim1, num_hidden_channels, hidden_channels_dims):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.hidden_channels_dims = hidden_channels_dims\n",
    "        self.conv1 = GCNConv(dim1, hidden_channels_dims[0],normalize=False)\n",
    "        self.conv2 = GCNConv(hidden_channels_dims[0], hidden_channels_dims[1],normalize=False)\n",
    "        self.conv3 = GCNConv(hidden_channels_dims[1], hidden_channels_dims[2],normalize=False)\n",
    "        self.lin1 = Linear(hidden_channels_dims[-1], 2)\n",
    "\n",
    "    def forward(self, x1, edge_index1, edge_weight1, batch1):\n",
    "\n",
    "        x1= self.conv1(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv2(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv3(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "\n",
    "        x1 = global_mean_pool(x1, batch1)\n",
    "        x1 = self.lin1(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation acc doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation acc improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation acc improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_acc_max = 0\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "\n",
    "        score = val_acc\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        '''Saves model when validation acc increase.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation acc increased ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_acc_max = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  \n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_acc(model,checkpt_path,loader,seeds):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        model.to(device)\n",
    "    accs = []\n",
    "    for i in range(1,seeds+1):\n",
    "        checkpt_path = checkpt_path\n",
    "        model.load_state_dict(torch.load(checkpt_path+f\"/checkpoint_seed_{i}.pt\"))\n",
    "        x = test(loader)\n",
    "        accs.append(x)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(dim1 = 116, num_hidden_channels = 3, hidden_channels_dims = [32, 16, 8])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_channels = 3\n",
    "hidden_channels_dims = [32,24,16]\n",
    "activation = 'ReLU'\n",
    "Patience = 20\n",
    "Dropout = 0.5 \n",
    "Learning_rate = 0.001\n",
    "\n",
    "for threshold_percentage in [0.5,1,1.5,2,4,6,8,10,12]:\n",
    "    start = time.time()\n",
    "\n",
    "    th_p,th_n = get_threshold_value(list(AD_dict.values()), list(CN_dict.values()), 'correlation', threshold_percentage)\n",
    "  \n",
    "    AD_graph={}\n",
    "    CN_graph={}\n",
    "    for sub_id in (AD_dict):\n",
    "        AD_graph[sub_id] = create_graph(timeseries=AD_dict[sub_id], threshold_p=th_p, threshold_n=th_n, y=0)\n",
    "    for sub_id in (CN_dict):\n",
    "        CN_graph[sub_id] = create_graph(timeseries=CN_dict[sub_id], threshold_p=th_p, threshold_n=th_n, y=1)\n",
    "    \n",
    "    final_train_accs = []\n",
    "    final_test_accs = []\n",
    "    final_val_accs = []\n",
    "    all_train_accs = []\n",
    "    all_val_accs = []\n",
    "    all_test_accs = []\n",
    "\n",
    "    seeds=100\n",
    "    for i in range(1,seeds+1):\n",
    "        # print(i)\n",
    "        random.seed(i)\n",
    "        np.random.seed(i)\n",
    "\n",
    "        train_data = [AD_graph[sub_id] for sub_id in AD_train[i-1]] + [CN_graph[sub_id] for sub_id in CN_train[i-1]]\n",
    "        train_label = [0]*len(AD_train[i-1]) + [1]*len(CN_train[i-1])\n",
    "        train_data,train_label = shuffle(train_data, train_label, random_state=i)\n",
    "        \n",
    "        val_data = [AD_graph[sub_id] for sub_id in AD_val[i-1]] + [CN_graph[sub_id] for sub_id in CN_val[i-1]]\n",
    "        val_label = [0]*len(AD_val[i-1]) + [1]*len(CN_val[i-1])\n",
    "        val_data, val_label = shuffle(val_data, val_label, random_state=i)\n",
    "\n",
    "        test_data = [AD_graph[sub_id] for sub_id in AD_test[i-1]] + [CN_graph[sub_id] for sub_id in CN_test[i-1]]\n",
    "        test_label = [0]*len(AD_test[i-1]) + [1]*len(CN_test[i-1])\n",
    "        test_data,test_label = shuffle(test_data, test_label, random_state=i)\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#         w = torch.Tensor([2.2727,1]).cuda()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        epochs=200\n",
    "        \n",
    "        os.makedirs(f\"manual_thr/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{threshold_percentage}\", exist_ok=True)\n",
    "        checkpt_path = f\"manual_thr/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{threshold_percentage}/checkpoint_seed_{i}.pt\"\n",
    "        early_stopping = EarlyStopping(patience=Patience, verbose=True, path=checkpt_path)\n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            train(model)\n",
    "            train_acc = test(train_loader)\n",
    "            val_acc = test(val_loader)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            early_stopping(val_acc, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch:{epoch}\")\n",
    "#                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "                print(f'GCN graph classification Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {early_stopping.val_acc_max:.4f}')\n",
    "                break\n",
    "            if epoch==epochs:\n",
    "                print(f'GCN graph classification Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {early_stopping.val_acc_max:.4f}')\n",
    "#                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "        \n",
    "        plt.plot(train_accs,label='train')\n",
    "        plt.plot(val_accs,label='val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        print('seed no.:',i)\n",
    "        print(\"GCN Final accuracy average = \", sum(final_val_accs)/len(final_val_accs))\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            model.to(device)\n",
    "        checkpt_path = f\"manual_thr/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{threshold_percentage}\"\n",
    "        model.load_state_dict(torch.load(checkpt_path+f\"/checkpoint_seed_{i}.pt\"))\n",
    "        final_train_accs.append(test(train_loader))\n",
    "        final_test_accs.append(test(test_loader))\n",
    "        final_val_accs.append(test(val_loader))\n",
    "                            \n",
    "    with open(f\"manual_thr/{activation}/hidden:{hidden_channels_dims}/checkpoints_th:{threshold_percentage}/report\", \"w\") as file:\n",
    "        file.write(f\"num_hidden_channels={num_hidden_channels}\\n\"\n",
    "                   f\"hidden_channels_dims={hidden_channels_dims}\\n\"\n",
    "                   f\"Activation={activation}\\n\"\n",
    "                   f\"Patience={Patience}\\n\"\n",
    "                   f\"Dropout={Dropout}\\n\"\n",
    "                   f\"Learning_rate={Learning_rate}\\n\"\n",
    "                   f\"seeds={seeds}\\n\"\n",
    "                   f\"epochs={epochs}\\n\"\n",
    "                   f\"Threshold percentage={threshold_percentage}%\\n\"\n",
    "                   f\"threshold_pos={th_p:0.2f}\\n\"\n",
    "                   f\"threshold_neg={th_n:0.2f}\\n\"\n",
    "                   f\"train_avg_accuracy={stat.mean(final_train_accs)*100:0.2f}%\\n\"\n",
    "                   f\"train_accuracy_stdev={stat.stdev(final_train_accs)*100:0.2f}%\\n\"\n",
    "                   f\"val_avg_accuracy={stat.mean(final_val_accs)*100:0.2f}%\\n\"\n",
    "                   f\"val_accuracy_stdev={stat.stdev(final_val_accs)*100:0.2f}%\\n\"\n",
    "                   f\"test_avg_accuracy={stat.mean(final_test_accs)*100:0.2f}%\\n\"\n",
    "                   f\"test_accuracy_stdev={stat.stdev(final_test_accs)*100:0.2f}%\\n\")\n",
    "    end = time.time()\n",
    "    with open(\"time_report\",\"a\") as f:\n",
    "        f.write(f\"time_{threshold_percentage} = {end-start:0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
