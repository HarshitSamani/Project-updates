{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dict = load_obj('AAL_data/timeseries/AD')\n",
    "CN_dict = load_obj('AAL_data/timeseries/CN')\n",
    "\n",
    "AD_train = load_obj('AAL_data/AD_train_full')\n",
    "AD_val = load_obj('AAL_data/AD_val_full')\n",
    "AD_test = load_obj('AAL_data/AD_test_full')\n",
    "\n",
    "CN_train = load_obj('AAL_data/CN_train_full')\n",
    "CN_val = load_obj('AAL_data/CN_val_full')\n",
    "CN_test = load_obj('AAL_data/CN_test_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_corr_mat(train_series):    \n",
    "    all_correlation_matrices = []\n",
    "    for timeseries in train_series:\n",
    "        all_correlation_matrices.append(get_correlation_matrix(timeseries,'correlation'))\n",
    "\n",
    "    avg_correlation_matrices = np.mean(np.array(all_correlation_matrices),axis=0)\n",
    "    return avg_correlation_matrices\n",
    "\n",
    "def partitions(avg_correlation_matrices,seed=None):\n",
    "    correlation_matrix = avg_correlation_matrices\n",
    "    G = nx.from_numpy_matrix(np.array(np.abs(correlation_matrix)),create_using=nx.Graph)\n",
    "    partition = community_louvain.best_partition(G,random_state=seed)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(timeseries,msr):\n",
    "    correlation_measure = ConnectivityMeasure(kind=msr)\n",
    "    correlation_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "    return correlation_matrix\n",
    "\n",
    "def get_upper_triangular_matrix(matrix):\n",
    "    upp_mat = []\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(i+1,len(matrix)):\n",
    "            upp_mat.append(matrix[i][j])\n",
    "    return upp_mat\n",
    "\n",
    "    \n",
    "# def get_adj_mat(correlation_matrix, threshold_value, weighted = True):\n",
    "#     adj_mat = []\n",
    "#     for i in correlation_matrix:\n",
    "#         row = []\n",
    "#         for j in i:\n",
    "#             if abs(j)>threshold_value:\n",
    "#                 if not weighted:\n",
    "#                     row.append(1)\n",
    "#                 else:\n",
    "#                     row.append(abs(j))\n",
    "#             else:\n",
    "#                 row.append(0)\n",
    "#         adj_mat.append(row)\n",
    "#     return adj_mat\n",
    "\n",
    "\n",
    "def get_adj_mat(correlation_matrix, th_inter, th_intra, partition):\n",
    "    adj_mat = []\n",
    "    k=0\n",
    "    for i in range(len(correlation_matrix)):\n",
    "        row = []\n",
    "                   \n",
    "        for j in range(len(correlation_matrix[0])):\n",
    "                       \n",
    "            if partition[i]==partition[j]:\n",
    "                if abs(correlation_matrix[i][j])>th_inter:\n",
    "                    row.append(abs(correlation_matrix[i][j]))\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            \n",
    "            else:\n",
    "                if abs(correlation_matrix[i][j])>th_intra:\n",
    "                    row.append(abs(correlation_matrix[i][j]))\n",
    "                else:\n",
    "                    row.append(0)\n",
    "                \n",
    "    \n",
    "        adj_mat.append(row)\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "def connect_isolated_nodes(adj_mat, correlation_matrix):\n",
    "    correlation_matrix = list(np.array(correlation_matrix) - np.array(np.eye(len(correlation_matrix))))\n",
    "    correlation_matrix = [list(a) for a in correlation_matrix]\n",
    "                              \n",
    "    for row_num in range(len(adj_mat)):\n",
    "        if sum(adj_mat[row_num]) == 0:\n",
    "            index_max_element_corr_row = correlation_matrix[row_num].index(max(correlation_matrix[row_num]))\n",
    "            adj_mat[row_num][index_max_element_corr_row] = 1\n",
    "    return adj_mat\n",
    "    \n",
    "# def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "#     ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "#     cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "#     ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "#     cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "#     all_correlation_values = ad_upper + cn_upper\n",
    "#     all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "\n",
    "#     all_correlation_values = np.array([abs(i) for i in all_correlation_values])\n",
    "#     all_correlation_values = np.sort(all_correlation_values)[::-1]\n",
    "\n",
    "#     th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "#     return all_correlation_values[int(th_val_index)]\n",
    "\n",
    "\n",
    "# def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "#     ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "#     cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "#     ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "#     cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "#     all_correlation_values = ad_upper + cn_upper\n",
    "#     all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "    \n",
    "#     all_correlation_values_pos=[]\n",
    "#     all_correlation_values_neg=[]\n",
    "#     for i in all_correlation_values:\n",
    "#         if i==1:\n",
    "#             continue\n",
    "#         elif i>0:\n",
    "#             all_correlation_values_pos.append(i)\n",
    "#         else:  \n",
    "#             all_correlation_values_neg.append(abs(i))\n",
    "\n",
    "#     all_correlation_values_pos = np.array(all_correlation_values_pos)\n",
    "#     all_correlation_values_pos = np.sort(all_correlation_values_pos)[::-1]\n",
    "    \n",
    "#     all_correlation_values_neg = np.array(all_correlation_values_neg)\n",
    "#     all_correlation_values_neg = np.sort(all_correlation_values_neg)[::-1]\n",
    "\n",
    "#     th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "    \n",
    "#     return all_correlation_values_pos[int(th_val_index)], all_correlation_values_neg[int(th_val_index)]\n",
    "\n",
    "def create_graph(timeseries, th_inter, th_intra, y, partition, measure='correlation'):\n",
    "    correlation_matrix = get_correlation_matrix(timeseries, measure)\n",
    "    adj_mat = get_adj_mat(correlation_matrix, th_inter, th_intra, partition)\n",
    "\n",
    "    G = nx.from_numpy_matrix(np.array(adj_mat), create_using=nx.DiGraph)\n",
    "    data=torch_geometric.utils.from_networkx(G)\n",
    "    data['x'] = torch.tensor(correlation_matrix, dtype=torch.float)\n",
    "    data['y'] = torch.tensor([y])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "        data = data.to(device)\n",
    "        return data\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, dim1, num_hidden_channels, hidden_channels_dims):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.hidden_channels_dims = hidden_channels_dims\n",
    "        self.conv1 = GCNConv(dim1, hidden_channels_dims[0])\n",
    "        self.conv2 = GCNConv(hidden_channels_dims[0], hidden_channels_dims[1])\n",
    "        self.conv3 = GCNConv(hidden_channels_dims[1], hidden_channels_dims[2])\n",
    "        self.lin1 = Linear(hidden_channels_dims[-1], 2)\n",
    "\n",
    "    def forward(self, x1, edge_index1, edge_weight1, batch1):\n",
    "\n",
    "        x1= self.conv1(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv2(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "        \n",
    "        x1 = self.conv3(x1, edge_index1, edge_weight1)\n",
    "        x1 = x1.relu()\n",
    "#         x1 = x1.tanh()\n",
    "        x1 = F.dropout(x1, p=0.5, training=self.training)\n",
    "\n",
    "        x1 = global_mean_pool(x1, batch1)\n",
    "        x1 = self.lin1(x1)\n",
    "        x1 = torch.softmax(x1,dim=1)\n",
    "        \n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation acc doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation acc improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation acc improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_acc_max = 0\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_acc, model):\n",
    "\n",
    "        score = val_acc\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        '''Saves model when validation acc increase.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation acc increased ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_acc_max = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  \n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "def f1(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.weight, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        y_pred += pred.cpu().detach().tolist()\n",
    "        y_true += data.y.cpu().detach().tolist()\n",
    "\n",
    "    return f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_acc(model,checkpt_path,loader,seeds):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "        model.to(device)\n",
    "    accs = []\n",
    "    for i in range(1,seeds+1):\n",
    "        checkpt_path = checkpt_path\n",
    "        model.load_state_dict(torch.load(checkpt_path+f\"/checkpoint_seed_{i}.pt\"))\n",
    "        x = test(loader)\n",
    "        accs.append(x)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(dim1 = 116, num_hidden_channels = 3, hidden_channels_dims = [32, 16,8])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_channels = 3\n",
    "hidden_channels_dims = [32,24,16]\n",
    "activation = 'ReLU'\n",
    "Patience = 200\n",
    "Dropout = 0.5 # Dropout\n",
    "Learning_rate = 0.001\n",
    "\n",
    "\n",
    "avg_correlation_matrix = get_avg_corr_mat(list(AD_dict.values()) + list(CN_dict.values()))\n",
    "partition = partitions(avg_correlation_matrix)\n",
    "\n",
    "for th_inter in [0.1,0.3,0.5,0.7,0.9]:\n",
    "    for th_intra in [0.1,0.3,0.5,0.7,0.9]:\n",
    "        start = time.time()\n",
    "\n",
    "        AD_graph={}\n",
    "        CN_graph={}\n",
    "        for sub_id in (AD_dict):\n",
    "            AD_graph[sub_id] = create_graph(timeseries=AD_dict[sub_id], th_inter=th_inter, th_intra=th_intra, y=0, partition=partition)\n",
    "        for sub_id in (CN_dict):\n",
    "            CN_graph[sub_id] = create_graph(timeseries=CN_dict[sub_id], th_inter=th_inter, th_intra=th_intra, y=1, partition=partition)\n",
    "\n",
    "\n",
    "        final_train_accs = []\n",
    "        final_test_accs = []\n",
    "        final_val_accs = []\n",
    "        all_train_accs = []\n",
    "        all_val_accs = []\n",
    "        all_test_accs = []\n",
    "        final_train_f1s = []\n",
    "        final_val_f1s = []\n",
    "        final_test_f1s = []\n",
    "    \n",
    "\n",
    "        seeds=100\n",
    "        for i in range(1,seeds+1):\n",
    "            # print(i)\n",
    "            random.seed(i)\n",
    "            np.random.seed(i)\n",
    "            \n",
    "            \n",
    "            AD_train_data = [AD_graph[sub_id] for sub_id in AD_train[i-1]]\n",
    "            CN_train_data = [CN_graph[sub_id] for sub_id in CN_train[i-1]]\n",
    "            residual = len(CN_train_data) - 2*len(AD_train_data)\n",
    "            extra = random.sample(AD_train_data, residual)\n",
    "            AD_train_data = AD_train_data*2 + extra\n",
    "\n",
    "            train_data = AD_train_data + CN_train_data\n",
    "            train_label = [0]*len(AD_train_data) + [1]*len(CN_train_data)\n",
    "            train_data,train_label = shuffle(train_data, train_label, random_state=i)\n",
    "\n",
    "            val_data = [AD_graph[sub_id] for sub_id in AD_val[i-1]] + [CN_graph[sub_id] for sub_id in CN_val[i-1]]\n",
    "            val_label = [0]*len(AD_val[i-1]) + [1]*len(CN_val[i-1])\n",
    "            val_data, val_label = shuffle(val_data, val_label, random_state=i)\n",
    "\n",
    "            test_data = [AD_graph[sub_id] for sub_id in AD_test[i-1]] + [CN_graph[sub_id] for sub_id in CN_test[i-1]]\n",
    "            test_label = [0]*len(AD_test[i-1]) + [1]*len(CN_test[i-1])\n",
    "            test_data,test_label = shuffle(test_data, test_label, random_state=i)\n",
    "\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            test_losses = []\n",
    "            \n",
    "            train_f1s = []\n",
    "            val_f1s = []\n",
    "            test_f1s = []\n",
    "            \n",
    "            train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "            val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "            test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "            model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims)\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device('cuda:1')\n",
    "                model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            epochs=200\n",
    "\n",
    "            os.makedirs(f\"manual_community_f1_OVERSAMPLE/{activation}/hidden:{hidden_channels_dims}/th_inter:{th_inter},th_intra:{th_intra}\", exist_ok=True)\n",
    "            checkpt_path = f\"manual_community_f1_OVERSAMPLE/{activation}/hidden:{hidden_channels_dims}/th_inter:{th_inter},th_intra:{th_intra}/checkpoint_seed_{i}.pt\"\n",
    "            early_stopping = EarlyStopping(patience=Patience, verbose=True, path=checkpt_path)\n",
    "\n",
    "            for epoch in range(1,epochs+1):\n",
    "                train(model)\n",
    "                train_f1 = f1(train_loader)\n",
    "                val_f1 = f1(val_loader)\n",
    "                train_f1s.append(train_f1)\n",
    "                val_f1s.append(val_f1)\n",
    "                early_stopping(val_f1, model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping at epoch:{epoch}\")\n",
    "    #                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "                    print(f'GCN graph classification Epoch: {epoch:03d}, Train f1: {train_f1:.4f}, Val f1: {early_stopping.val_acc_max:.4f}')\n",
    "                    break\n",
    "                if epoch==epochs:\n",
    "                    print(f'GCN graph classification Epoch: {epoch:03d}, Train f1: {train_f1:.4f}, Val f1: {early_stopping.val_acc_max:.4f}')\n",
    "    #                 final_val_accs.append(early_stopping.val_acc_max)\n",
    "\n",
    "            plt.plot(train_f1s,label='train')\n",
    "            plt.plot(val_f1s,label='val')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    #         all_train_accs.append(train_accs)\n",
    "    #         all_val_accs.append(val_accs)\n",
    "\n",
    "\n",
    "#             print('seed no.:',i)\n",
    "#             print(\"GCN Final accuracy average = \", sum(final_val_accs)/len(final_val_accs))\n",
    "#             print('\\n\\n')\n",
    "            \n",
    "            model = GNN(dim1=116, num_hidden_channels=num_hidden_channels, hidden_channels_dims=hidden_channels_dims)\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device('cuda:1')\n",
    "                model.to(device)\n",
    "            checkpt_path = f\"manual_community_f1_OVERSAMPLE/{activation}/hidden:{hidden_channels_dims}/th_inter:{th_inter},th_intra:{th_intra}\"\n",
    "            model.load_state_dict(torch.load(checkpt_path+f\"/checkpoint_seed_{i}.pt\"))\n",
    "            \n",
    "            final_train_accs.append(test(train_loader))\n",
    "            final_test_accs.append(test(test_loader))\n",
    "            final_val_accs.append(test(val_loader))\n",
    "\n",
    "            final_train_f1s.append(f1(train_loader))\n",
    "            final_test_f1s.append(f1(test_loader))\n",
    "            final_val_f1s.append(f1(val_loader))\n",
    "\n",
    "\n",
    "        with open(f\"manual_community_f1_OVERSAMPLE/{activation}/hidden:{hidden_channels_dims}/th_inter:{th_inter},th_intra:{th_intra}/report\", \"w\") as file:\n",
    "            file.write(f\"num_hidden_channels={num_hidden_channels}\\n\"\n",
    "                       f\"hidden_channels_dims={hidden_channels_dims}\\n\"\n",
    "                       f\"Activation={activation}\\n\"\n",
    "                       f\"Patience={Patience}\\n\"\n",
    "                       f\"Dropout={Dropout}\\n\"\n",
    "                       f\"Learning_rate={Learning_rate}\\n\"\n",
    "                       f\"seeds={seeds}\\n\"\n",
    "                       f\"epochs={epochs}\\n\"\n",
    "                       f\"Threshold inter={th_inter}\\n\"\n",
    "                       f\"threshold intra={th_intra}\\n\"\n",
    "                       f\"train_avg_accuracy={stat.mean(final_train_accs)*100:0.2f}%\\n\"\n",
    "                       f\"train_accuracy_stdev={stat.stdev(final_train_accs)*100:0.2f}%\\n\"\n",
    "                       f\"train_avg_f1={stat.mean(final_train_f1s):0.2f}\\n\"\n",
    "                       f\"train_f1_stdev={stat.stdev(final_train_f1s):0.2f}\\n\"\n",
    "                       f\"val_avg_accuracy={stat.mean(final_val_accs)*100:0.2f}%\\n\"\n",
    "                       f\"val_accuracy_stdev={stat.stdev(final_val_accs)*100:0.2f}%\\n\"\n",
    "                       f\"val_avg_f1={stat.mean(final_val_f1s):0.2f}\\n\"\n",
    "                       f\"val_f1_stdev={stat.stdev(final_val_f1s):0.2f}\\n\"\n",
    "                       f\"test_avg_accuracy={stat.mean(final_test_accs)*100:0.2f}%\\n\"\n",
    "                       f\"test_accuracy_stdev={stat.stdev(final_test_accs)*100:0.2f}%\\n\"\n",
    "                       f\"test_avg_f1={stat.mean(final_test_f1s):0.2f}\\n\"\n",
    "                       f\"test_f1_stdev={stat.stdev(final_test_f1s):0.2f}\\n\")\n",
    "        end = time.time()\n",
    "        with open(\"time_report\",\"a\") as f:\n",
    "            f.write(f\"time_th_inter:{th_inter},th_intra:{th_intra} = {end-start:0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
