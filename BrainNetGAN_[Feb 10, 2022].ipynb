{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):\n",
    "    with open(path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dict = load_obj('AAL_data/timeseries/AD')\n",
    "CN_dict = load_obj('AAL_data/timeseries/CN')\n",
    "\n",
    "AD_train = load_obj('AAL_data/AD_train_full')\n",
    "AD_val = load_obj('AAL_data/AD_val_full')\n",
    "AD_test = load_obj('AAL_data/AD_test_full')\n",
    "\n",
    "CN_train = load_obj('AAL_data/CN_train_full')\n",
    "CN_val = load_obj('AAL_data/CN_val_full')\n",
    "CN_test = load_obj('AAL_data/CN_test_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(timeseries,msr):\n",
    "    correlation_measure = ConnectivityMeasure(kind=msr)\n",
    "    correlation_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "    return correlation_matrix\n",
    "\n",
    "def get_upper_triangular_matrix(matrix):\n",
    "    upp_mat = []\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(i+1,len(matrix)):\n",
    "            upp_mat.append(matrix[i][j])\n",
    "    return upp_mat\n",
    "\n",
    "    \n",
    "# def get_adj_mat(correlation_matrix, threshold_value, weighted = True):\n",
    "#     adj_mat = []\n",
    "#     for i in correlation_matrix:\n",
    "#         row = []\n",
    "#         for j in i:\n",
    "#             if abs(j)>threshold_value:\n",
    "#                 if not weighted:\n",
    "#                     row.append(1)\n",
    "#                 else:\n",
    "#                     row.append(abs(j))\n",
    "#             else:\n",
    "#                 row.append(0)\n",
    "#         adj_mat.append(row)\n",
    "#     return adj_mat\n",
    "\n",
    "\n",
    "def get_adj_mat(correlation_matrix, th_value_p, th_value_n):\n",
    "    adj_mat = []\n",
    "    k=0\n",
    "    for i in correlation_matrix:\n",
    "        row = []\n",
    "        for j in i:\n",
    "            if j>0:\n",
    "                if j > th_value_p:\n",
    "                    row.append(j)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            else:\n",
    "                if abs(j) > th_value_n:\n",
    "                    row.append(abs(j))\n",
    "                else:\n",
    "                    row.append(0)\n",
    "        adj_mat.append(row)\n",
    "#     adj_mat = connect_isolated_nodes(adj_mat, correlation_matrix)\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "def connect_isolated_nodes(adj_mat, correlation_matrix):\n",
    "    correlation_matrix = list(np.array(correlation_matrix) - np.array(np.eye(len(correlation_matrix))))\n",
    "    correlation_matrix = [list(a) for a in correlation_matrix]\n",
    "                              \n",
    "    for row_num in range(len(adj_mat)):\n",
    "        if sum(adj_mat[row_num]) == 0:\n",
    "            index_max_element_corr_row = correlation_matrix[row_num].index(max(correlation_matrix[row_num]))\n",
    "            adj_mat[row_num][index_max_element_corr_row] = 1\n",
    "    return adj_mat\n",
    "    \n",
    "# def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "#     ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "#     cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "#     ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "#     cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "#     all_correlation_values = ad_upper + cn_upper\n",
    "#     all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "\n",
    "#     all_correlation_values = np.array([abs(i) for i in all_correlation_values])\n",
    "#     all_correlation_values = np.sort(all_correlation_values)[::-1]\n",
    "\n",
    "#     th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "#     return all_correlation_values[int(th_val_index)]\n",
    "\n",
    "\n",
    "def get_threshold_value(ad_timeseires, cn_timeseries, measure, threshold_percent):\n",
    "    ad_corr_mats = [get_correlation_matrix(ts, measure) for ts in ad_timeseires]\n",
    "    cn_corr_mats = [get_correlation_matrix(ts, measure) for ts in cn_timeseries]\n",
    "\n",
    "    ad_upper = [get_upper_triangular_matrix(matrix) for matrix in ad_corr_mats]\n",
    "    cn_upper = [get_upper_triangular_matrix(matrix) for matrix in cn_corr_mats]\n",
    "\n",
    "    all_correlation_values = ad_upper + cn_upper\n",
    "    all_correlation_values = np.array(all_correlation_values).flatten()\n",
    "    \n",
    "    all_correlation_values_pos=[]\n",
    "    all_correlation_values_neg=[]\n",
    "    for i in all_correlation_values:\n",
    "        if i==1:\n",
    "            continue\n",
    "        elif i>0:\n",
    "            all_correlation_values_pos.append(i)\n",
    "        else:  \n",
    "            all_correlation_values_neg.append(abs(i))\n",
    "\n",
    "    all_correlation_values_pos = np.array(all_correlation_values_pos)\n",
    "    all_correlation_values_pos = np.sort(all_correlation_values_pos)[::-1]\n",
    "    \n",
    "    all_correlation_values_neg = np.array(all_correlation_values_neg)\n",
    "    all_correlation_values_neg = np.sort(all_correlation_values_neg)[::-1]\n",
    "\n",
    "    th_val_index = (len(all_correlation_values)*threshold_percent)//100\n",
    "    \n",
    "    return all_correlation_values_pos[int(th_val_index)], all_correlation_values_neg[int(th_val_index)]\n",
    "\n",
    "def create_graph(timeseries, threshold_p, threshold_n, y, measure='correlation'):\n",
    "    correlation_matrix = get_correlation_matrix(timeseries, measure)\n",
    "    adj_mat = get_adj_mat(correlation_matrix, threshold_p, threshold_n)\n",
    "\n",
    "    G = nx.from_numpy_matrix(np.array(adj_mat), create_using=nx.DiGraph)\n",
    "    data=torch_geometric.utils.from_networkx(G)\n",
    "    data['x'] = torch.tensor(correlation_matrix, dtype=torch.float)\n",
    "    data['y'] = torch.tensor([y])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "        data = data.to(device)\n",
    "        return data\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Linear(4096, 6670),\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "#     def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "#         return nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "#             ),\n",
    "#             nn.InstanceNorm2d(out_channels, affine=True),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#         )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_d, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(noise_d + num_classes, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 6670),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def symm_reshape(x):\n",
    "        return nn.triu(x)\n",
    "        \n",
    "#         return nn.tril(x)\n",
    "        \n",
    "\n",
    "    def forward(self, x, labels):\n",
    "#         x = torch.tensor(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, labels], dim=1)\n",
    "        x = self.net(x)\n",
    "#         x = self.symm_reshape(x)\n",
    "        return \n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes,example,bias=False):\n",
    "        super(E2EBlock, self).__init__()\n",
    "        self.d = example.size(3)\n",
    "        self.cnn1 = torch.nn.Conv2d(in_planes,planes,(1,self.d),bias=bias)\n",
    "#         self.cnn1 = torch.nn.Conv2d(in_planes,planes,(1,self.d),bias=bias)\n",
    "        self.cnn2 = torch.nn.Conv2d(in_planes,planes,(self.d,1),bias=bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.cnn1(x)\n",
    "        b = self.cnn2(x)\n",
    "        return torch.cat([a]*self.d,3)+torch.cat([b]*self.d,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainNetCNN(torch.nn.Module):\n",
    "    def __init__(self, example, num_classes=10):\n",
    "        super(BrainNetCNN, self).__init__()\n",
    "        self.in_planes = example.size(1)\n",
    "        self.d = example.size(3)\n",
    "        \n",
    "        self.e2econv1 = E2EBlock(1,32,example,bias=True)\n",
    "        self.e2econv2 = E2EBlock(32,64,example,bias=True)\n",
    "#         self.N2G = torch.nn.Conv2d(1,256,(self.d,1))\n",
    "#         self.E2N = torch.nn.Conv2d(64,1,(1,self.d))\n",
    "        self.N2G = torch.nn.Conv2d(1,256,(self.d,1))\n",
    "        self.dense1 = torch.nn.Linear(256,128)\n",
    "        self.dense2 = torch.nn.Linear(128,30)\n",
    "        self.dense3 = torch.nn.Linear(30,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.e2econv1(x),negative_slope=0.33)\n",
    "#         out = F.leaky_relu(self.e2econv2(out),negative_slope=0.33) \n",
    "        out = F.leaky_relu(self.E2N(out),negative_slope=0.33)\n",
    "        out = F.dropout(F.leaky_relu(self.N2G(out),negative_slope=0.33),p=0.5)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         out = F.dropout(F.leaky_relu(self.dense1(out),negative_slope=0.33),p=0.5)\n",
    "        out = F.dropout(F.leaky_relu(self.dense2(out),negative_slope=0.33),p=0.5)\n",
    "        out = F.leaky_relu(self.dense3(out),negative_slope=0.33)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, B).to(device)\n",
    "    interpolated_mat = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    mixed_scores = critic(interpolated_mat)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_mat,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "#             critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "            \n",
    "#         for _ in range(CRITIC_ITERATIONS):\n",
    "#             noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "#             fake = gen(noise)\n",
    "# #             critic_real = critic(real).reshape(-1)\n",
    "#             critic_fake = critic(fake).reshape(-1)\n",
    "#             gp = gradient_penalty(critic, real, fake, device=device)\n",
    "#             loss_critic = (\n",
    "#                 -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "#             )\n",
    "#             critic.zero_grad()\n",
    "#             loss_critic.backward(retain_graph=True)\n",
    "#             opt_critic.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
